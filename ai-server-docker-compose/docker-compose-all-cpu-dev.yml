x-default-opts: &default-opts
  restart: unless-stopped
  tty: true
  stdin_open: true
  privileged: false
  ipc: private

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ai_server_sample_ollama
    <<: *default-opts
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    networks:
      - server_network
  fastapi:
    build:
      context: ../
      dockerfile: dockerfile/dockerfile.fastapi
    container_name: ai_server_sample_fastapi
    <<: *default-opts
    volumes:
      - ../fastapi:/app
    networks:
      - server_network
    entrypoint: 
      - /bin/sh
      - -c
      - |
        echo "Starting FastAPI server..."
        chmod +x entrypoint.sh
        ./entrypoint.sh

  nextjs:
    image: node:22.14.0-bookworm
    container_name: ai_server_sample_nextjs
    <<: *default-opts
    volumes:
      - ../nextjs:/app
    networks:
      - server_network
    entrypoint: /bin/sh -c "cd /app && npm install && npm run dev"

  nginx:
    image: nginx:stable-perl
    container_name: ai_server_sample_nginx
    <<: *default-opts
    ports:
      - ${NGINX_PORT}:80
    volumes:
      - ../nginx/nginx.conf:/etc/nginx/nginx.conf
      - ../nginx/dev.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - fastapi
      - nextjs
    networks:
      - server_network

  searxng_valkey:
    container_name: searxng_valkey
    image: docker.io/valkey/valkey:8-alpine
    command: valkey-server --save 30 1 --loglevel warning
    <<: *default-opts
    networks:
      - server_network
    volumes:
      - valkey-data2:/data
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  searxng:
    container_name: searxng
    build:
      context: ../
      dockerfile: dockerfile/dockerfile.searxng
    <<: *default-opts
    networks:
      - server_network
    volumes:
      - searxng_cache:/var/cache/searxng
      - searxng_data:/etc/searxng
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
      - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
      - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
  
  mcp:
    container_name: ai_server_sample_mcp
    build:
      context: ../
      dockerfile: dockerfile/dockerfile.mcp
    <<: *default-opts
    ports:
      - "61080:8000"
    networks:
      - server_network
    volumes:
      - ../mcp:/app
    entrypoint: 
      - /bin/sh
      - -c
      - |
        echo "Starting MCP server..."
        chmod +x entrypoint.sh
        ./entrypoint.sh

networks:
  server_network:

volumes:
  ollama:
  valkey-data2:
  searxng_cache:
  searxng_data: